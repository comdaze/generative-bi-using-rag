diff -ruN /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/.env /home/ec2-user/generative-bi-using-rag/application/.env
--- /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/.env	1970-01-01 00:00:00.000000000 +0000
+++ /home/ec2-user/generative-bi-using-rag/application/.env	2025-04-08 11:49:17.962322251 +0000
@@ -0,0 +1,42 @@
+RDS_MYSQL_USERNAME=llmdata
+RDS_MYSQL_PASSWORD=llmdata
+RDS_MYSQL_HOST=mysql-db
+RDS_MYSQL_PORT=3306
+RDS_MYSQL_DBNAME=llm
+
+# possible value: 'service', 'docker'. Will route to Secrets Manager if set to 'service'. Will use env vars below if set to 'docker'
+OPENSEARCH_TYPE=service
+AOS_HOST=opensearch-node1
+AOS_PORT=9200
+AOS_AWS_REGION=us-east-1
+AOS_DOMAIN=llm-data-analytics
+AOS_INDEX=uba
+AOS_INDEX_NER=uba_ner
+AOS_INDEX_AGENT=uba_agent
+AOS_USER=admin
+AOS_PASSWORD=admin
+
+BEDROCK_REGION=us-east-1
+RDS_REGION_NAME=us-east-1
+AWS_DEFAULT_REGION=us-east-1
+DYNAMODB_AWS_REGION=us-east-1
+
+EMBEDDING_DIMENSION=1536
+BEDROCK_EMBEDDING_MODEL=amazon.titan-embed-text-v1
+
+# If you need to use ak/sk to access bedrock, please configure bedrock's ak/sk to Secrets Manager, Examples are as follows
+# BEDROCK_SECRETS_AK_SK=bedrock-ak-sk
+
+BEDROCK_SECRETS_AK_SK=
+
+OPENSEARCH_SECRETS_URL_HOST=opensearch-host-url
+OPENSEARCH_SECRETS_USERNAME_PASSWORD=opensearch-master-user
+
+# SAGEMAKER_ENDPOINT_EMBEDDING=
+
+
+VITE_COGNITO_REGION=
+VITE_COGNITO_USER_POOL_ID=
+VITE_COGNITO_USER_POOL_WEB_CLIENT_ID=
+
+ENABLE_USER_PROFILE_MAP=False
\ No newline at end of file
diff -ruN /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/Dockerfile /home/ec2-user/generative-bi-using-rag/application/Dockerfile
--- /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/Dockerfile	2025-02-21 09:11:33.000000000 +0000
+++ /home/ec2-user/generative-bi-using-rag/application/Dockerfile	2025-04-08 11:56:26.366357722 +0000
@@ -1,4 +1,4 @@
-FROM public.ecr.aws/docker/library/python:3.10-slim
+FROM python:3.10-slim
 
 WORKDIR /app
 
diff -ruN /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/config_files/stauth_config.yaml /home/ec2-user/generative-bi-using-rag/application/config_files/stauth_config.yaml
--- /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/config_files/stauth_config.yaml	2025-02-21 09:11:33.000000000 +0000
+++ /home/ec2-user/generative-bi-using-rag/application/config_files/stauth_config.yaml	2025-02-25 01:20:48.962362511 +0000
@@ -1,15 +1,15 @@
 credentials:
   usernames:
     admin:
-      email: amazon@amazon.com
+      email: zhihay@amazon.com
       failed_login_attempts: 0 # Will be managed automatically
       logged_in: False # Will be managed automatically
       name: AWS
-      password: $2b$12$NDQv5NLaWiVlNuzQYHwAo.tv.f.TuX1nbdoUZi44/Y3xv4I4QAfjy # Set the password following instructions in README
+      password: $2b$12$63KcDNxFVV98oCWMjGNHbunZ9V1KsHOYG8Vt6UeNehTdH00KfVdTC # Set the password following instructions in README
 cookie:
   expiry_days: 2
   key: some_signature_key # Must be string
   name: some_cookie_name
 pre-authorized:
   emails:
-    - amazon@amazon.com
+    - zhihay@amazon.com
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/nlq/__pycache__/__init__.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/nlq/__pycache__/__init__.cpython-39.pyc differ
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/nlq/business/__pycache__/__init__.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/nlq/business/__pycache__/__init__.cpython-39.pyc differ
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/nlq/business/__pycache__/model.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/nlq/business/__pycache__/model.cpython-39.pyc differ
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/nlq/business/__pycache__/profile.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/nlq/business/__pycache__/profile.cpython-39.pyc differ
diff -ruN /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/nlq/core/state_machine.py /home/ec2-user/generative-bi-using-rag/application/nlq/core/state_machine.py
--- /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/nlq/core/state_machine.py	2025-02-21 09:11:33.000000000 +0000
+++ /home/ec2-user/generative-bi-using-rag/application/nlq/core/state_machine.py	2025-04-09 16:37:15.193979226 +0000
@@ -195,9 +195,18 @@
                 each_item_dict["_source"] = each_entity["_source"]
                 if "vector_field" in each_item_dict["_source"]:
                     del each_item_dict["_source"]["vector_field"]
-                entity_retrieve.append(each_item_dict)
+                if each_entity['_score'] > 0.8:
+                    entity_retrieve.append(each_item_dict)
                 if each_entity['_source']['entity_count'] > 1 and each_entity['_score'] > 0.98:
                     same_name_entity[each_entity['_source']['entity']] = each_entity['_source']['entity_table_info']
+            # 保存实体信息到answer对象中
+            self.answer.ask_entity_select.entity_retrieval = entity_retrieve
+        
+            # 如果是knowledge_search意图，直接转向KNOWLEDGE_SEARCH状态
+            if self.answer.query_intent == "knowledge_search":
+                self.transition(QueryState.KNOWLEDGE_SEARCH)
+                return
+                               
             if len(same_name_entity) > 0 and self.answer.query_intent == "normal_search":
                 for key, value in same_name_entity.items():
                     change_value = []
@@ -378,7 +387,10 @@
             self.answer.query_intent = "reject_search"
             self.transition(QueryState.REJECT_INTENT)
         elif self.knowledge_search_flag:
-            self.transition(QueryState.KNOWLEDGE_SEARCH)
+            #self.transition(QueryState.KNOWLEDGE_SEARCH)
+            self.answer.query_intent = "knowledge_search"
+            # 先进入实体检索状态
+            self.transition(QueryState.ENTITY_RETRIEVAL)
         elif self.agent_intent_flag:
             self.answer.query_intent = "agent_search"
             self.transition(QueryState.AGENT_TASK)
@@ -395,10 +407,14 @@
 
     @log_execution
     def handle_knowledge_search(self):
+        # 使用已经保存在 answer 对象中的实体信息
+        entity_info = self.answer.ask_entity_select.entity_retrieval
         response, model_response = knowledge_search(search_box=self.context.query_rewrite,
                                                     model_id=self.context.model_type,
                                                     prompt_map=self.context.database_profile["prompt_map"],
-                                                    environment_dict=self.context.database_profile["prompt_environment"])
+                                                    environment_dict=self.context.database_profile["prompt_environment"],
+                                                    entity_info=entity_info  # 添加实体信息参数
+                                                    )
         self.token_info[QueryState.KNOWLEDGE_SEARCH.name] = model_response.token_info
         self.answer.query = self.context.search_box
         self.answer.query_rewrite = self.context.query_rewrite
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/nlq/data_access/__pycache__/__init__.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/nlq/data_access/__pycache__/__init__.cpython-39.pyc differ
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/nlq/data_access/__pycache__/dynamo_model.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/nlq/data_access/__pycache__/dynamo_model.cpython-39.pyc differ
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/nlq/data_access/__pycache__/dynamo_profile.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/nlq/data_access/__pycache__/dynamo_profile.cpython-39.pyc differ
diff -ruN "/home/ec2-user/guidance-for-generative-bi-v1.9.0/application/pages/9_\360\237\252\231_SageMaker_Model_Management.py" "/home/ec2-user/generative-bi-using-rag/application/pages/9_\360\237\252\231_SageMaker_Model_Management.py"
--- "/home/ec2-user/guidance-for-generative-bi-v1.9.0/application/pages/9_\360\237\252\231_SageMaker_Model_Management.py"	1970-01-01 00:00:00.000000000 +0000
+++ "/home/ec2-user/generative-bi-using-rag/application/pages/9_\360\237\252\231_SageMaker_Model_Management.py"	2025-02-22 01:05:11.528701845 +0000
@@ -0,0 +1,198 @@
+import json
+import boto3
+import streamlit as st
+from dotenv import load_dotenv
+
+from nlq.business.model import ModelManagement
+from nlq.business.profile import ProfileManagement
+from utils.logging import getLogger
+from utils.navigation import make_sidebar
+
+logger = getLogger()
+
+
+def new_connection_clicked():
+    st.session_state.new_sagemaker_mode = True
+    st.session_state.update_sagemaker_mode = False
+    st.session_state.current_model = None
+
+
+def model_connect(sagemaker_name, sagemaker_region, prompt_template, input_payload, output_format):
+    connect_flag = False
+    connect_info = "-1"
+    try:
+        if sagemaker_name.startswith("sagemaker."):
+            sagemaker_name = sagemaker_name[10:]
+        system_prompt = "You are a human friendly conversation assistant."
+        user_prompt = "Hello, who are you"
+        prompt = prompt_template.replace("SYSTEM_PROMPT", system_prompt).replace("USER_PROMPT", user_prompt)
+        input_payload = json.loads(input_payload)
+        input_payload_text = json.dumps(input_payload)
+        input_payload_text = input_payload_text.replace("\"INPUT\"", json.dumps(prompt))
+        sagemaker_client = boto3.client(service_name='sagemaker-runtime', region_name=sagemaker_region)
+        response = sagemaker_client.invoke_endpoint(
+            EndpointName=sagemaker_name,
+            Body=input_payload_text,
+            ContentType="application/json",
+        )
+        response = json.loads(response.get('Body').read())
+        answer = eval(output_format)
+        connect_info = answer
+        connect_flag = True
+    except Exception as e:
+        logger.error("Failed to connect: {}".format(e))
+        connect_info = str(e)
+    return connect_flag, connect_info
+
+
+def test_model_connect(sagemaker_name, sagemaker_region, prompt_template, input_payload, output_format):
+    if st.button('Model Connection Test'):
+        if sagemaker_name == '':
+            st.error("SageMaker Endpoint is required!")
+        elif sagemaker_region == '':
+            st.error("SageMaker region is required!")
+        elif input_payload == '':
+            st.error("Input payload is required!")
+        elif output_format == '':
+            st.error("Output format is required!")
+        connect_flag, connect_info = model_connect(sagemaker_name, sagemaker_region, prompt_template, input_payload,
+                                                   output_format)
+        if connect_flag:
+            st.success(f"Connected successfully!")
+        else:
+            st.error(f"Failed to connect!")
+        st.write(connect_info)
+
+
+def main():
+    load_dotenv()
+
+    st.set_page_config(page_title="SageMaker Model Management")
+    make_sidebar()
+
+    if 'new_sagemaker_mode' not in st.session_state:
+        st.session_state['new_sagemaker_mode'] = False
+
+    if 'update_sagemaker_mode' not in st.session_state:
+        st.session_state['update_sagemaker_mode'] = False
+
+    if 'current_model' not in st.session_state:
+        st.session_state['current_model'] = None
+
+    if "samaker_model" not in st.session_state:
+        st.session_state.samaker_model = ModelManagement.get_all_models()
+
+    with st.sidebar:
+        st.title("SageMaker Model Management")
+        st.selectbox("SageMaker Model", st.session_state.samaker_model,
+                     index=None,
+                     placeholder="Please SageMaker Model...", key='current_sagemaker_name')
+        if st.session_state.current_sagemaker_name:
+            st.session_state.current_model = ModelManagement.get_model_by_id(st.session_state.current_sagemaker_name)
+            st.session_state.update_sagemaker_mode = True
+            st.session_state.new_sagemaker_mode = False
+
+        st.button('Create New SageMaker Model', on_click=new_connection_clicked)
+
+    if st.session_state.new_sagemaker_mode:
+        st.subheader("New SageMaker Model")
+        sagemaker_name = st.text_input("SageMaker Endpoint Name")
+        sagemaker_region = st.text_input("SageMaker Endpoint Region")
+        prompt_template = st.text_area("Prompt Template",
+                                       placeholder="Enter prompt template, need contain SYSTEM_PROMPT Placeholder and USER_PROMPT Placeholder. \n For Example: SYSTEM_PROMPT<|im_start|>user\nUSER_PROMPT<|im_end|>\n<|im_start|>assistant\n",
+                                       height=200,
+                                       help="Enter prompt template, need contain SYSTEM_PROMPT Placeholder and USER_PROMPT Placeholder")
+        example_input = {"inputs": "INPUT", "parameters": {"max_new_tokens": 256}}
+        input_payload = st.text_area("Mode Input Payload",
+                                     placeholder="Enter input payload in JSON dumps str, The input text use INPUT Placeholder. For Example: " + json.dumps(
+                                         example_input),
+                                     height=200,
+                                     help="Enter input payload in JSON dumps str, The input text use INPUT Placeholder")
+        output_format = st.text_area("Model Output Format",
+                                     placeholder="Enter output format, The output value name is response. For Example: response[0]['generated_text']",
+                                     height=100,
+                                     help="Enter output format, The output value name is response")
+
+        test_model_connect(sagemaker_name, sagemaker_region, prompt_template, input_payload, output_format)
+
+        if st.button('Add Connection', type='primary'):
+            if sagemaker_name == '':
+                st.error("SageMaker name is required!")
+            elif sagemaker_region == '':
+                st.error("SageMaker region is required!")
+            elif input_payload == '':
+                st.error("Input payload is required!")
+            elif output_format == '':
+                st.error("Output format is required!")
+            else:
+                ModelManagement.add_model(model_id="sagemaker." + sagemaker_name, model_region=sagemaker_region,
+                                          prompt_template=prompt_template, input_payload=input_payload,
+                                          output_format=output_format)
+                st.success(f"{sagemaker_name} added successfully!")
+                st.session_state.samaker_model.append("sagemaker." + sagemaker_name)
+                st.session_state.new_connection_mode = False
+
+                with st.spinner('Update Prompt...'):
+                    all_profiles = ProfileManagement.get_all_profiles_with_info()
+                    for item in all_profiles:
+                        profile_name = item
+                        profile_value = all_profiles[profile_name]
+                        profile_prompt_map = profile_value["prompt_map"]
+                        update_prompt_map = {}
+                        for each_process in profile_prompt_map:
+                            update_prompt_map[each_process] = profile_prompt_map[each_process]
+                            update_prompt_map[each_process]["system_prompt"][sagemaker_name] = profile_prompt_map[each_process]["system_prompt"]["sonnet-20240229v1-0"]
+                            update_prompt_map[each_process]["user_prompt"][sagemaker_name] = profile_prompt_map[each_process]["user_prompt"]["sonnet-20240229v1-0"]
+                        ProfileManagement.update_prompt_map(profile_name, update_prompt_map)
+                    st.success("Prompt added successfully!")
+
+
+    elif st.session_state.update_sagemaker_mode:
+        st.subheader("Update SageMaker Connection")
+        current_model = st.session_state.current_model
+        sagemaker_name = st.text_input("SageMaker Endpoint Name", current_model.model_id, disabled=True)
+        sagemaker_region = st.text_input("SageMaker Endpoint Region", current_model.model_region, disabled=True)
+        prompt_template = st.text_area("Prompt Template", current_model.prompt_template, height=200)
+        input_payload = st.text_area("Mode Input Payload", current_model.input_payload, height=200)
+        output_format = st.text_area("Model Output Format", current_model.output_format, height=100)
+        test_model_connect(sagemaker_name, sagemaker_region, prompt_template, input_payload, output_format)
+        if st.button('Update Model Connection', type='primary'):
+            ModelManagement.update_model(model_id=sagemaker_name, model_region=sagemaker_region,
+                                         prompt_template=prompt_template, input_payload=input_payload,
+                                         output_format=output_format)
+            st.success(f"{sagemaker_name} updated successfully!")
+
+        if st.button('Delete Model Connection'):
+            ModelManagement.delete_model(sagemaker_name)
+            st.success(f"{sagemaker_name} deleted successfully!")
+            if sagemaker_name in st.session_state.samaker_model:
+                st.session_state.samaker_model.remove(sagemaker_name)
+            st.session_state.current_model = None
+            with st.spinner('Delete Prompt...'):
+                all_profiles = ProfileManagement.get_all_profiles_with_info()
+                if sagemaker_name.startswith("sagemaker."):
+                    sagemaker_name = sagemaker_name[10:]
+                for item in all_profiles:
+                    profile_name = item
+                    profile_value = all_profiles[profile_name]
+                    profile_prompt_map = profile_value["prompt_map"]
+                    update_prompt_map = {}
+                    for each_process in profile_prompt_map:
+                        update_prompt_map[each_process] = profile_prompt_map[each_process]
+                        if sagemaker_name in update_prompt_map[each_process]["system_prompt"]:
+                            del update_prompt_map[each_process]["system_prompt"][sagemaker_name]
+                        if sagemaker_name in update_prompt_map[each_process]["user_prompt"]:
+                            del update_prompt_map[each_process]["user_prompt"][sagemaker_name]
+                    ProfileManagement.update_prompt_map(profile_name, update_prompt_map)
+                st.success("Prompt added successfully!")
+
+
+        st.session_state.update_sagemaker_mode = False
+
+    else:
+        st.subheader("SageMaker Model Management")
+        st.info('Please select model in the left sidebar.')
+
+
+if __name__ == '__main__':
+    main()
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/utils/__pycache__/__init__.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/utils/__pycache__/__init__.cpython-39.pyc differ
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/utils/__pycache__/logging.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/utils/__pycache__/logging.cpython-39.pyc differ
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/utils/__pycache__/navigation.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/utils/__pycache__/navigation.cpython-39.pyc differ
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/utils/__pycache__/prompt.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/utils/__pycache__/prompt.cpython-39.pyc differ
diff -ruN /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/utils/llm.py /home/ec2-user/generative-bi-using-rag/application/utils/llm.py
--- /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/utils/llm.py	2025-02-21 09:11:33.000000000 +0000
+++ /home/ec2-user/generative-bi-using-rag/application/utils/llm.py	2025-04-09 16:53:04.918121926 +0000
@@ -388,7 +388,28 @@
     return query_rewrite_result, model_response
 
 
-def knowledge_search(model_id, search_box, prompt_map, environment_dict=None):
+def knowledge_search(model_id, search_box, prompt_map, environment_dict=None, entity_info=None):
+    # 创建环境字典的副本，以便我们可以添加实体信息
+    if environment_dict is None:
+        environment_dict = {}
+    else:
+        environment_dict = environment_dict.copy()
+    
+    # 如果有实体信息，构建实体信息字符串
+    entity_info_str = ""
+    if entity_info and len(entity_info) > 0:
+        entity_info_str = "\n\n相关实体信息：\n"
+        for entity in entity_info:
+            entity_source = entity.get("_source", {})
+            entity_name = entity_source.get("entity", "")
+            entity_comment = entity_source.get("comment", "")
+            entity_type = entity_source.get("entity_type", "")
+            entity_info_str += f"- 实体名称: {entity_name}, 类型: {entity_type}, 描述: {entity_comment}\n"
+    
+    # 将实体信息添加到环境字典中
+    environment_dict["entity_info"] = entity_info_str
+    
+    # 生成提示并调用模型
     user_prompt, system_prompt = generate_knowledge_prompt(prompt_map, search_box, model_id, environment_dict)
     max_tokens = 2048
     model_response = invoke_llm_model(model_id, system_prompt, user_prompt, max_tokens, False)
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/utils/prompts/__pycache__/__init__.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/utils/prompts/__pycache__/__init__.cpython-39.pyc differ
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/utils/prompts/__pycache__/generate_prompt.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/utils/prompts/__pycache__/generate_prompt.cpython-39.pyc differ
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/utils/prompts/__pycache__/guidance_prompt.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/utils/prompts/__pycache__/guidance_prompt.cpython-39.pyc differ
Binary files /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/utils/prompts/__pycache__/table_prompt.cpython-39.pyc and /home/ec2-user/generative-bi-using-rag/application/utils/prompts/__pycache__/table_prompt.cpython-39.pyc differ
diff -ruN /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/utils/prompts/generate_prompt.py /home/ec2-user/generative-bi-using-rag/application/utils/prompts/generate_prompt.py
--- /home/ec2-user/guidance-for-generative-bi-v1.9.0/application/utils/prompts/generate_prompt.py	2025-02-21 09:11:33.000000000 +0000
+++ /home/ec2-user/generative-bi-using-rag/application/utils/prompts/generate_prompt.py	2025-04-09 16:54:53.965149958 +0000
@@ -1074,8 +1074,11 @@
 <rules>
 1. answer should as concise as possible
 2. if you don't know the answer to the question, just answer you don't know.
+3. if entity information is provided, use it to enhance your answer
 </rules>
 
+{entity_info}
+
 <context>
 Here is a list of acronyms and their full names plus some comments, which may help you understand the context of the question.
 [{'Acronym': 'NDDC', 'Full name': 'Direct Digital Commerce'},
